# P2 ‚Äî Python-ML-Algorithms (Clasificaci√≥n + Clustering)

Esta carpeta **`P2/`** contiene la **Pr√°ctica 2** del repositorio. Ampl√≠a la Pr√°ctica 1 a√±adiendo **nuevos algoritmos de aprendizaje autom√°tico** y t√©cnicas de **clustering no supervisado**.

Incluye:

- **Clasificadores supervisados**:
  - KNN (implementaci√≥n propia)
  - Naive Bayes (nominal + continuo gaussiano)
  - **Regresi√≥n Log√≠stica** entrenada con descenso de gradiente estoc√°stico
- **Aprendizaje no supervisado**:
  - **K-Means** con inicializaci√≥n tipo *k-means++*, c√°lculo de inercia y visualizaci√≥n
- **Carga y preprocesado de datos** (codificaci√≥n de nominales, estandarizaci√≥n)
- **Estrategias de validaci√≥n**: *hold-out* y **k-fold cross-validation**
- Notebook **`MainP2.ipynb`** con experimentos y comparativas

> Este proyecto forma parte de un repositorio con varias pr√°cticas.  
> La **Pr√°ctica 1** se encuentra en la carpeta `P1/`.

---

## üìÅ Estructura de la carpeta `P2/`

```
P2/
‚îú‚îÄ‚îÄ Datos.py
‚îú‚îÄ‚îÄ EstrategiaParticionado.py
‚îú‚îÄ‚îÄ Clasificador.py
‚îú‚îÄ‚îÄ ClasificadorKNN.py
‚îú‚îÄ‚îÄ ClasificadorNB.py
‚îú‚îÄ‚îÄ ClasificadorRL.py
‚îú‚îÄ‚îÄ KMeans.py
‚îî‚îÄ‚îÄ MainP2.ipynb
```

---

## üß© ¬øQu√© hace cada archivo?

### `Datos.py`
- Carga un CSV con `pandas`.
- Detecta atributos nominales y **trata siempre la √∫ltima columna como clase**.
- Construye:
  - `datos.datos` ‚Üí matriz `numpy` con todo num√©rico.
  - `datos.nominalAtributos` ‚Üí lista `bool` indicando qu√© columnas son nominales.
  - `datos.diccionarios` ‚Üí diccionarios para codificar valores nominales.
- `extraeDatos(idx)` devuelve subconjuntos por √≠ndices.
- `estandarizarDatos()` aplica estandarizaci√≥n z-score a atributos continuos.

### `EstrategiaParticionado.py`
- Define `Particion(indicesTrain, indicesTest)`.
- Implementa:
  - `ValidacionSimple(proporcionTrain, numeroEjecuciones)` (*hold-out*).
  - `ValidacionCruzada(k)` (*k-fold cross-validation*).

### `Clasificador.py`
- Clase base abstracta con:
  - `entrenamiento(...)`
  - `clasifica(...)`
- Implementa:
  - `validacion(...)`: ejecuta train/test sobre cada partici√≥n.
  - `error(...)`: calcula la **tasa de error** y acumula **matriz de confusi√≥n media**.

### `ClasificadorKNN.py`
- Implementaci√≥n propia de **KNN**:
  - Distancia eucl√≠dea.
  - Normalizaci√≥n opcional usando estad√≠sticas del train.
  - Predicci√≥n por mayor√≠a entre los `k` vecinos m√°s cercanos.

### `ClasificadorNB.py`
- Implementaci√≥n de **Naive Bayes**:
  - A-prioris `P(clase)` por frecuencia.
  - Atributos nominales: conteo de `P(valor | clase)` (Laplace opcional).
  - Atributos continuos: modelo **gaussiano** (media y desviaci√≥n).
  - Predicci√≥n por m√°xima probabilidad posterior.

### `ClasificadorRL.py`
- Implementaci√≥n de **Regresi√≥n Log√≠stica binaria**:
  - Inicializa pesos aleatoriamente.
  - Entrena con **descenso de gradiente estoc√°stico** durante un n√∫mero de √©pocas.
  - Usa funci√≥n sigmoide para obtener probabilidades.
  - Clasifica con umbral 0.5.
  - Incluye `obtener_scores(...)` para devolver probabilidades (√∫til para curvas ROC, etc.).

### `KMeans.py`
- Implementaci√≥n de **K-Means**:
  - Inicializaci√≥n de centroides con **k-means++**.
  - Reasignaci√≥n iterativa de puntos a centroides.
  - Re-c√°lculo de centroides hasta convergencia (tolerancia `tol` o `max_iter`).
  - C√°lculo de **inercia** (suma de distancias cuadradas intra-cl√∫ster).
  - `plot_clusters(...)` para visualizaci√≥n en 2D.
  - `predict(...)` para asignar nuevos puntos a los cl√∫steres aprendidos.

### `MainP2.ipynb`
- Notebook de experimentaci√≥n:
  - Comparaci√≥n de clasificadores (KNN, NB, RL).
  - Evaluaci√≥n con validaci√≥n simple y cruzada.
  - Pruebas de K-Means y visualizaci√≥n de cl√∫steres.
  - An√°lisis de resultados.

---

## üß† Flujo general de uso (clasificaci√≥n)

1) Cargar dataset con `Datos`.  
2) Elegir estrategia de particionado (`ValidacionSimple` o `ValidacionCruzada`).  
3) Elegir clasificador (`ClasificadorKNN`, `ClasificadorNB` o `ClasificadorRL`).  
4) Ejecutar `Clasificador.validacion(...)` para entrenar y evaluar en cada partici√≥n.  

Se obtienen:
- Vector de tasas de error por partici√≥n
- Matriz de confusi√≥n media

---

## ‚ñ∂Ô∏è C√≥mo ejecutar

### Opci√≥n recomendada: Notebook

Abrir:

```bash
MainP2.ipynb
```

y ejecutar las celdas (Jupyter / VS Code).

Ah√≠ encontrar√°s:
- Carga de datos
- Entrenamiento de clasificadores
- Validaci√≥n y m√©tricas
- Ejemplos de K-Means y gr√°ficas

---

## üßØ Notas t√©cnicas

- La **Regresi√≥n Log√≠stica** est√° pensada para **problemas binarios** (clases 0/1).
- En **Naive Bayes**, las medias/desviaciones de continuos se calculan por atributo (no por clase), lo que simplifica la implementaci√≥n.
- En **KNN**, si alg√∫n atributo tiene desviaci√≥n 0, habr√≠a divisi√≥n por cero (no est√° controlado).
- En **ValidacionSimple**, el par√°metro `numeroEjecuciones` no genera m√∫ltiples particiones (solo una).

---

## üõ†Ô∏è Dependencias

- `numpy`
- `pandas`
- `scipy`
- `scikit-learn`
- `matplotlib`
- Jupyter Notebook

---

## üë§ Autor

Santiago de Prada Lorenzo
